{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67349b-5ee8-4301-a1cd-01f4c758d4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install numexpr\n",
    "# !pip install langchain\n",
    "# !pip install openai\n",
    "# !pip install openai-whisper\n",
    "# !pip install sentence-transformers\n",
    "# !pip install unstructured\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa32158-b701-484f-b956-e5081c188bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import random\n",
    "import re\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0164df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = \"enter_your_api_key\"\n",
    "\n",
    "# Set the OpenAI API key as an environment variable\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c219fd4-cd9d-4c88-b8c9-dd9477e5b81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chains import LLMChain, LLMMathChain, SequentialChain, TransformChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.tools import Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bd8e7-23b0-49a6-8590-3b04c0e4d656",
   "metadata": {},
   "source": [
    "# Langchain 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a8456-d7ab-4fde-9be5-6eb13eb72b6a",
   "metadata": {},
   "source": [
    "We first create a `PromptTemplate` class and initialize a templat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb60c4a9-624e-461b-88ad-24912c19d4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# user question\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "581124ac-afda-4896-8fcd-1fd3dc48407c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Which NFL team won the Super Bowl in the 2010 season?\\n\\nAnswer: '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a73ce74b-40c8-4667-8d15-0b4626bfeeca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"text-davinci-003\"\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f9cab1-20b6-47a2-a2ff-2790f0e066a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca10b9ab-8603-4998-978c-4545059b6159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Green Bay Packers won the Super Bowl in the 2010 season.\n"
     ]
    }
   ],
   "source": [
    "# ask the user question about NFL 2010\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19155b80-e1f3-4d48-a662-0ec872dcab0a",
   "metadata": {},
   "source": [
    "Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8099189e-248e-4904-a3fc-bb6852e08831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {\"question\": \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {\"question\": \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {\"question\": \"Who was the 12th person on the moon?\"},\n",
    "    {\"question\": \"How many eyes does a blade of grass have?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c789afb6-64ad-4ba2-a4b5-d679f3e7a5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94685b7-2348-4a29-86e6-f1e1194dfc13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Green Bay Packers won the Super Bowl in the 2010 season.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[0][0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4a0e5f8-40f5-4dae-b0ce-05edaa7946af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'193.04 centimeters'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[1][0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "141f8441-0817-4083-bbe1-667bf458b387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 12th person to walk on the moon was astronaut Alan Bean, who was part of the Apollo 12 mission in November 1969.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[2][0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "093be5eb-ab59-43d7-a833-b653e5a5038b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A blade of grass does not have any eyes.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[3][0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae47fe-170c-4582-9f54-b7a28f86cef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What are chains anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c8589-ac5a-46d9-8641-807ddf8d6973",
   "metadata": {},
   "source": [
    "Definition: Chains are one of the fundamental building blocks of this lib (as you can guess!).\n",
    "\n",
    "The official definition of chains is the following:\n",
    "\n",
    ">A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561c9bb-b610-4289-9af0-7d8398dddfc9",
   "metadata": {},
   "source": [
    "So a chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to an text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5181a-d810-4cd4-a671-ad2176f9ebb6",
   "metadata": {},
   "source": [
    "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
    "\n",
    "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
    "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451dbca6-a9bf-42a4-a33f-5baa08fcc81f",
   "metadata": {},
   "source": [
    "### Utility Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a0b8db-c0f7-43ca-8dc5-41344cb3fdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:56: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm_math = LLMMathChain(llm=model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ae3652a-d519-4990-a581-73306f9c6599",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "13**.3432\n",
      "```\n",
      "...numexpr.evaluate(\"13**.3432\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math.run(\"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc9841-3737-4e51-9e2e-703cb657c74c",
   "metadata": {},
   "source": [
    "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c66853-fab7-42ff-8256-1c0f860418fb",
   "metadata": {},
   "source": [
    "#### Enter Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0c362-cc0b-4df8-abc5-697d55110a0e",
   "metadata": {},
   "source": [
    "The question we send as input to the chain is not the only input that the llm recieves ðŸ˜‰. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a prompt. Let's see what this chain's prompt is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13a065d9-f456-49c2-b842-c243eb7ce2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee35a8-e405-4b7c-8ba3-e2796de3d51d",
   "metadata": {},
   "source": [
    "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems it should not try to do math on its own but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! ðŸ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c55fc7ac-082f-4dab-900b-dc5118830cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n2.907'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we set the prompt to only have the question we ask\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=model)\n",
    "\n",
    "# we ask the llm for the answer with no context\n",
    "\n",
    "llm_chain.run(\"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826ab96-eed1-4f0b-8cd4-fcd4a3d28782",
   "metadata": {},
   "source": [
    "Wrong answer! Herein lies the power of prompting and one of our most important insights so far:\n",
    "\n",
    "Insight: by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a51e5-d965-49f9-981b-749dc824d097",
   "metadata": {},
   "source": [
    "### Generic Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e1d5d-5f33-437d-92fd-b6a12cd36597",
   "metadata": {},
   "source": [
    "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488f96b-529c-4c4a-81da-de25ff52119d",
   "metadata": {},
   "source": [
    "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6862dfc-b18f-4a51-9d9a-fb6b33653976",
   "metadata": {},
   "source": [
    "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "216e06a8-6739-4d56-969b-69a115c9e79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "\n",
    "    # replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r\"(\\r\\n|\\r|\\n){2,}\", r\"\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    return {\"output_text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa864e6-57ff-4746-8745-bef9434cabf3",
   "metadata": {},
   "source": [
    "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b01b6119-9779-40a9-8eac-a8bce1f9505e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b1ba995-9223-40fd-be09-2aedabc5f6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A random text with some irregular spacing.\\n Another one here as well.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_extra_spaces_chain.run(\n",
    "    \"A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73788f34-50c3-4983-87ae-939790434e38",
   "metadata": {},
   "source": [
    "Great! Now things will get interesting.\n",
    "\n",
    "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the TransformChain does not use a llm so the styling will have to be done elsewhere. That's where our LLMChain comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f4b7709-aaee-4d15-99e7-80fb6f05bee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f91ba-e3fc-4979-96a9-f06e2aefca37",
   "metadata": {},
   "source": [
    "And next, initialize our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b17f19d6-adc5-4814-8494-5bfee66b062f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm=model, prompt=prompt, output_key=\"final_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec52eb-9dfd-4f45-9464-4193b4327a0d",
   "metadata": {},
   "source": [
    "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
    "\n",
    "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9663730f-f9f9-4ca0-9918-cb419f6edcae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(\n",
    "    chains=[clean_extra_spaces_chain, style_paraphrase_chain],\n",
    "    input_variables=[\"text\", \"style\"],\n",
    "    output_variables=[\"final_output\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149ab54-7a78-4269-b6a5-b5e1df9d62d5",
   "metadata": {},
   "source": [
    "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6538b907-a669-461f-8ba4-765523ddd7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Chains allow us to combine multiple \n",
    "\n",
    "\n",
    "components together to create a single, coherent application. \n",
    "\n",
    "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
    "\n",
    "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
    "\n",
    "\n",
    "combining chains with other components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe35989-f97c-4e96-811e-c3511f1a3461",
   "metadata": {},
   "source": [
    "We are all set. Time to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4baf9120-8dcd-4022-8eed-753619056c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chains bind us, let us join\n",
      "Components to make one app divine\n",
      "For instance, take user input, format it with a PromptTemplate\n",
      "Then pass the response to an LLM\n",
      "We can make more complex chains, by combining multiple or with other components too.\n"
     ]
    }
   ],
   "source": [
    "print(sequential_chain.run({\"text\": input_text, \"style\": \"a poet\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef7fad-46e5-4b02-abd0-2de7ffd9952b",
   "metadata": {},
   "source": [
    "## Langchain Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e214440-0f49-4d0a-9b79-db752f0edb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Define the data structure we want to be parsed out from the LLM response\n",
    "\n",
    "notice that the class contains a setup (a string) and a punchline (a string.\n",
    "The descriptions are used to construct the prompt to the llm. This particular\n",
    "example also has a validator which checks if the setup contains a question mark.\n",
    "\n",
    "from: https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "868183c3-0de5-4c76-8e71-271bda514b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Defining the query from the user\n",
    "\"\"\"\n",
    "joke_query = \"Tell me a joke about parrots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc6c94d8-dec3-4a4d-82a0-83a6179cc268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the user query.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n",
      "Tell me a joke about parrots\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Defining the prompt to the llm\n",
    "\n",
    "from: https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic\n",
    "\"\"\"\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=joke_query)\n",
    "\n",
    "print(_input.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d69c93e-5751-49ab-a6d7-5390732b33dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't parrots make good detectives?\", punchline=\"Because they're always repeating themselves!\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Declaring a model and querying it with the parser defined input\n",
    "\"\"\"\n",
    "\n",
    "output = model(_input.to_string())\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ac28e-4d50-421c-ab0c-44f43f476a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c4a7dfe-c66c-4b7e-a5ef-d5c84f929155",
   "metadata": {},
   "source": [
    "# LLMs as Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d1313d9-0bcb-4587-bb3d-78b0ce383349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the model used in this test\n",
    "model_name = \"text-davinci-003\"\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6ce8561-27f4-4150-ad7b-a3489aa23b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class sampleOutputTemplate(BaseModel):\n",
    "    output: str = Field(description=\"contact information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4598a-ac46-45ab-afe5-86cf5857f67a",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d4e2a85-dbc0-4168-91c6-6e6efa90d525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Defining utility functions for constructing a readable exchange\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def system_output(output):\n",
    "    \"\"\"Function for printing out to the user\"\"\"\n",
    "    print(\"======= Bot =======\")\n",
    "    print(output)\n",
    "\n",
    "\n",
    "def user_input():\n",
    "    \"\"\"Function for getting user input\"\"\"\n",
    "    print(\"======= Human Input =======\")\n",
    "    return input()\n",
    "\n",
    "\n",
    "def parsing_info(output):\n",
    "    \"\"\"Function for printing out key info\"\"\"\n",
    "    print(f\"*Info* {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae788b-b042-4e79-9597-e3cbb72a2bec",
   "metadata": {},
   "source": [
    "## Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b432cda-68af-4f4f-b0db-e2229f5133ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Edge:\n",
    "\n",
    "    \"\"\"Edge\n",
    "    at its highest level, an edge checks if an input is good, then parses\n",
    "    data out of that input if it is good\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, condition, parse_prompt, parse_class, llm, max_retrys=3, out_node=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        condition (str): a True/False question about the input\n",
    "        parse_query (str): what the parser whould be extracting\n",
    "        parse_class (Pydantic BaseModel): the structure of the parse\n",
    "        llm (LangChain LLM): the large language model being used\n",
    "        \"\"\"\n",
    "        self.condition = condition\n",
    "        self.parse_prompt = parse_prompt\n",
    "        self.parse_class = parse_class\n",
    "        self.llm = llm\n",
    "\n",
    "        # how many times the edge has failed, for any reason, for deciding to skip\n",
    "        # when successful this resets to 0 for posterity.\n",
    "        self.num_fails = 0\n",
    "\n",
    "        # how many retrys are acceptable\n",
    "        self.max_retrys = max_retrys\n",
    "\n",
    "        # the node the edge directs towards\n",
    "        self.out_node = out_node\n",
    "\n",
    "    def check(self, input):\n",
    "        \"\"\"ask the llm if the input satisfies the condition\"\"\"\n",
    "        validation_query = f\"following the output schema, does the input satisfy the condition?\\ninput:{input}\\ncondition:{self.condition}\"\n",
    "\n",
    "        class Validation(BaseModel):\n",
    "            is_valid: bool = Field(description=\"if the condition is satisfied\")\n",
    "\n",
    "        parser = PydanticOutputParser(pydantic_object=Validation)\n",
    "        input = f\"Answer the user query.\\n{parser.get_format_instructions()}\\n{validation_query}\\n\"\n",
    "        return parser.parse(self.llm(input)).is_valid\n",
    "\n",
    "    def parse(self, input):\n",
    "        \"\"\"ask the llm to parse the parse_class, based on the parse_prompt, from the input\"\"\"\n",
    "        parse_query = f'{self.parse_prompt}:\\n\\n\"{input}\"'\n",
    "        parser = PydanticOutputParser(pydantic_object=self.parse_class)\n",
    "        input = f\"Answer the user query.\\n{parser.get_format_instructions()}\\n{parse_query}\\n\"\n",
    "        return parser.parse(self.llm(input))\n",
    "\n",
    "    def execute(self, input):\n",
    "        \"\"\"Executes the entire edge\n",
    "        returns a dictionary:\n",
    "        {\n",
    "            continue: bool,       weather or not should continue to next\n",
    "            result: parse_class,  the parsed result, if applicable\n",
    "            num_fails: int         the number of failed attempts\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        # input did't make it past the input condition for the edge\n",
    "        if not self.check(input):\n",
    "            self.num_fails += 1\n",
    "            if self.num_fails >= self.max_retrys:\n",
    "                return {\"continue\": True, \"result\": None, \"num_fails\": self.num_fails}\n",
    "            return {\"continue\": False, \"result\": None, \"num_fails\": self.num_fails}\n",
    "\n",
    "        try:\n",
    "            # attempting to parse\n",
    "            self.num_fails = 0\n",
    "            return {\n",
    "                \"continue\": True,\n",
    "                \"result\": self.parse(input),\n",
    "                \"num_fails\": self.num_fails,\n",
    "            }\n",
    "        except:\n",
    "            # there was some error in parsing.\n",
    "            # note, using the retry or correction parser here might be a good idea\n",
    "            self.num_fails += 1\n",
    "            if self.num_fails >= self.max_retrys:\n",
    "                return {\"continue\": True, \"result\": None, \"num_fails\": self.num_fails}\n",
    "            return {\"continue\": False, \"result\": None, \"num_fails\": self.num_fails}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f0edce8-df67-4a67-a806-a42bc589c0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the query for the condition, and parse prompt\n",
    "condition = \"Does the input contain fruits?\"\n",
    "parse_prompt = \"extract only the fruits from the following text. Do not extract any food items besides pure fruits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76e75004-3356-4826-b0c0-1c1766cbd67f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the edge\n",
    "testEdge = Edge(\n",
    "    condition=condition,\n",
    "    parse_prompt=parse_prompt,\n",
    "    parse_class=sampleOutputTemplate,\n",
    "    llm=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dbb0dcb-a72e-4710-bbd0-d753a8ec44c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a sample input from the user\n",
    "sample_input = \"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6729e7bf-acf2-44de-8da6-41ab9f8b5237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== testing the condition functionality =====\n",
      "weather or not the input \n",
      "\"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\"\n",
      "satisfies the condition\n",
      "\"Does the input contain fruits?\"\n",
      "result: True\n"
     ]
    }
   ],
   "source": [
    "print(\"===== testing the condition functionality =====\")\n",
    "print(\n",
    "    f'weather or not the input \\n\"{sample_input}\"\\nsatisfies the condition\\n\"{condition}\"'\n",
    ")\n",
    "print(\"result: {}\".format(testEdge.check(sample_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27ce6bb8-35e0-4f9d-89b7-ffbacd0ec35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== parse results =====\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-ZZs7Rw1ObMQMMWL9FpbFQxMB on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== parse results =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtestEdge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moutput)\n",
      "Cell \u001b[1;32mIn[40], line 48\u001b[0m, in \u001b[0;36mEdge.parse\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     46\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_class)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer the user query.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mparser\u001b[38;5;241m.\u001b[39mget_format_instructions()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mparse_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\langchain\\llms\\base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     )\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    877\u001b[0m         [prompt],\n\u001b[0;32m    878\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    879\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    880\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    881\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    883\u001b[0m     )\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    886\u001b[0m )\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\langchain\\llms\\base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         )\n\u001b[0;32m    644\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    646\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    654\u001b[0m         )\n\u001b[0;32m    655\u001b[0m     ]\n\u001b[1;32m--> 656\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    658\u001b[0m     )\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\langchain\\llms\\base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    543\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    545\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\langchain\\llms\\base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    523\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    528\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    530\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 531\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    532\u001b[0m                 prompts,\n\u001b[0;32m    533\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    534\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    535\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    536\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    537\u001b[0m             )\n\u001b[0;32m    538\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    539\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    540\u001b[0m         )\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\langchain\\llms\\openai.py:454\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    443\u001b[0m         {\n\u001b[0;32m    444\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m         }\n\u001b[0;32m    452\u001b[0m     )\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     response \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m_prompts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\langchain\\llms\\openai.py:114\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    116\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\_utils\\_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\resources\\completions.py:559\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1043\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1051\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1052\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1053\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[1;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    832\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    833\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    923\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    923\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTS\\REVIEWED PROJECTS\\LLM_2_customer_support\\customer_support_new\\customer_support\\csvn\\lib\\site-packages\\openai\\_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-ZZs7Rw1ObMQMMWL9FpbFQxMB on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "print(\"===== parse results =====\")\n",
    "print(testEdge.parse(sample_input).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673050f-5b67-4f0f-90b7-f9da00e3629b",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9263591-6f55-4214-b8d4-fc87bed92463",
   "metadata": {},
   "source": [
    "Now that we have an Edge, which handles input validation and parsing, we can define a Node, which handles conversational state. The Node requests a user for input, and passes that input to the directed edges coming from that Node. If none of the edges execute successfully, the Node asks the user for the input again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce4fa11-e8f9-477c-9c33-01eb0c5b6443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    \"\"\"Node\n",
    "    at its highest level, a node asks a user for some input, and trys\n",
    "    that input on all edges. It also manages and executes all\n",
    "    the edges it contains\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prompt, retry_prompt):\n",
    "        \"\"\"\n",
    "        prompt (str): what to ask the user\n",
    "        retry_prompt (str): what to ask the user if all edges fail\n",
    "        parse_class (Pydantic BaseModel): the structure of the parse\n",
    "        llm (LangChain LLM): the large language model being used\n",
    "        \"\"\"\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.retry_prompt = retry_prompt\n",
    "        self.edges = []\n",
    "\n",
    "    def run_to_continue(self, _input):\n",
    "        \"\"\"Run all edges until one continues\n",
    "        returns the result of the continuing edge, or None\n",
    "        \"\"\"\n",
    "        for edge in self.edges:\n",
    "            res = edge.execute(_input)\n",
    "            if res[\"continue\"]:\n",
    "                return res\n",
    "        return None\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"Handles the current conversational state\n",
    "        prompots the user, tries again, runs edges, etc.\n",
    "        returns the result from an adge\n",
    "        \"\"\"\n",
    "\n",
    "        # initial prompt for the conversational state\n",
    "        system_output(self.prompt)\n",
    "\n",
    "        while True:\n",
    "            # getting users input\n",
    "            _input = user_input()\n",
    "\n",
    "            # running through edges\n",
    "            res = self.run_to_continue(_input)\n",
    "\n",
    "            if res is not None:\n",
    "                # parse successful\n",
    "                parsing_info(f\"parse results: {res}\")\n",
    "                return res\n",
    "\n",
    "            # unsuccessful, prompting retry\n",
    "            system_output(self.retry_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f99797-a64f-49ea-b681-8e6b637cfb47",
   "metadata": {},
   "source": [
    "With this implemented, we can begin seeing conversations take place. Weâ€™ll implement a Node which requests contact information, and two edges: one which attempts to parse out a valid email, and one that attempts to parse out a valid phone number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f8c11-1d0a-4fae-b59c-486fda7c27aa",
   "metadata": {},
   "source": [
    "## Connecting Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012920d9-c60a-4d25-ab22-29a16eed3670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining 2 edges from the node\n",
    "\n",
    "condition1 = \"Does the input contain a full and valid email?\"\n",
    "parse_prompt1 = \"extract the email from the following text.\"\n",
    "edge1 = Edge(condition1, parse_prompt1, sampleOutputTemplate, model)\n",
    "condition2 = (\n",
    "    \"Does the input contain a full and valid phone number (xxx-xxx-xxxx or xxxxxxxxxx)?\"\n",
    ")\n",
    "parse_prompt2 = \"extract the phone number from the following text.\"\n",
    "edge2 = Edge(condition2, parse_prompt2, sampleOutputTemplate, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca7454-b6db-47d7-9174-b3e77c4432ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining A Node\n",
    "test_node = Node(\n",
    "    prompt=\"Please input your full email address or phone number\",\n",
    "    retry_prompt=\"I'm sorry, I didn't understand your response.\\nPlease provide a full email address or phone number(in the format xxx-xxx-xxxx)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5c026-10a5-4b68-8973-728a75ceddf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining Connections\n",
    "test_node.edges = [edge1, edge2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714a934-adfd-4e3c-a0a3-649c396313d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# running node. This handles all i/o and the logic to re-ask on failure.\n",
    "res = test_node.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddc960-8c07-4889-9524-f2575a79f546",
   "metadata": {},
   "source": [
    "# Customer Support flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee75c9e-9277-4b89-a10c-ba0b05d17743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data.chat import *\n",
    "from data.graph import *\n",
    "from data.validation import PhoneCallTicket, UserProfile, PhoneCallRequest\n",
    "from graph.chain_based_edge import *\n",
    "from graph.chain_based_node import *\n",
    "from graph.edge import BaseEdge\n",
    "from graph.node import BaseNode\n",
    "from graph.text_based_edge import PydanticTextBasedEdge\n",
    "from tools.rag_responder import HelpCenterAgent\n",
    "from tools.user_info_db import search_user_info_on_db, search_user_subscription_on_db\n",
    "from tools.audio_transcribe import call_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b6a521-618c-45e0-bf40-2bb959838d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27aa0134-1b04-47fa-8668-409e6263bbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_message_history(msg_input: str, role: Role):\n",
    "    message_history = MessageHistory([])\n",
    "    message_history.add_message(msg_input, role)\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b645d2d-55c8-42e5-8e12-084eb4706446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GreetingNode(BaseNode[str]):\n",
    "    STATIC_PROMPT = [\n",
    "        \"Hi, welcome to our online support, in order to proceed we need to identify you first, \"\n",
    "        \"could you please input your full email address or phone number\"\n",
    "    ]\n",
    "    RETRY_PROMPT = [\n",
    "        \"I'm sorry, I didn't understand your response.\"\n",
    "        \"\\nPlease provide a full email address or phone number(in the format xxx-xxx-xxxx)\"\n",
    "    ]\n",
    "\n",
    "    def greeting_message(self) -> Optional[MessageOutput]:\n",
    "        prompt = random.choice(self.STATIC_PROMPT)\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)\n",
    "\n",
    "    def no_edges_found(self, **kwargs) -> Optional[MessageOutput]:\n",
    "        prompt = random.choice(self.RETRY_PROMPT)\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181136a2-fefd-4d21-9e08-059aacef399b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BaseNode(abc.ABC, Generic[NodeInput]):\n",
      "\n",
      "    \"\"\"Node\n",
      "    at it's highest level, a node asks a user for some input, and trys\n",
      "    that input on all edges. It also manages and executes all\n",
      "    the edges it contains\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, edges: Optional[List[BaseEdge]] = None, final_state=False):\n",
      "        \"\"\"\n",
      "        prompt (str): what to ask the user\n",
      "        retry_prompt (str): what to ask the user if all edges fail\n",
      "        parse_class (Pydantic BaseModel): the structure of the parse\n",
      "        llm (LangChain LLM): the large language model being used\n",
      "        \"\"\"\n",
      "\n",
      "        self._edges = edges\n",
      "        self._node_input = None\n",
      "        self._final_state = final_state\n",
      "\n",
      "    def is_node_final(self):\n",
      "        return self._final_state\n",
      "\n",
      "    def set_node_input(self, edge_output: EdgeOutput):\n",
      "        self._node_input = edge_output\n",
      "\n",
      "    def run_to_continue(self, user_input: NodeInput) -> Optional[EdgeOutput]:\n",
      "        \"\"\"Run all edges until one continues\n",
      "        returns the result of the continuing edge, or None\n",
      "        \"\"\"\n",
      "        res = None\n",
      "        for edge in self._edges:\n",
      "            res = edge.execute(user_input)\n",
      "            if res is not None and res.should_continue:\n",
      "                return res\n",
      "        return res\n",
      "\n",
      "    def execute(self, user_input: NodeInput) -> Union[MessageOutput, EdgeOutput]:\n",
      "        \"\"\"Handles the current conversational state\n",
      "        prompts the user, tries again, runs edges, etc.\n",
      "        returns the result from an adge\n",
      "        \"\"\"\n",
      "        res = self.run_to_continue(user_input)\n",
      "        if res is None or not res.should_continue:\n",
      "            return self.no_edges_found(user_input)\n",
      "        else:\n",
      "            if res.next_node is not None:\n",
      "                res.next_node.set_node_input(res.result)\n",
      "\n",
      "        return res\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def greeting_message(self) -> Optional[MessageOutput]:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def no_edges_found(self, user_input: NodeInput) -> Optional[MessageOutput]:\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BaseNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2181210f-4d35-44d1-8318-698434c30f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "greeting_node = GreetingNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27c73bf-b623-4951-9083-1d0811890037",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MessageOutput(message='Hi, welcome to our online support, in order to proceed we need to identify you first, could you please input your full email address or phone number', role=<Role.ASSISTANT: 'assistant'>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greeting_node.greeting_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4968c8b9-9c59-48e5-84e6-93cdcd7d85d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UserInfoChainBasedEdge(ZeroShotChainBasedEdge):\n",
    "    _prompt_prefix = \"\"\"Your goal is to find out the user information and their subscription type.\n",
    "- You MUST ALWAYS combine the output of different tools to achieve your final answer.\n",
    "- The user subscription must be either free or premium, never empty\n",
    "\n",
    "To achieve this you have access to the following tools:\"\"\"\n",
    "\n",
    "    _prompt_suffix = \"\"\"\\nYour final answer should combine the information of previous Observations \n",
    "{format_instructions}\n",
    "Begin! \n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "    def _get_tools(self):\n",
    "        tools = [\n",
    "            Tool.from_function(\n",
    "                func=search_user_info_on_db,\n",
    "                description=\"Database tool to search user information, like user id, phone number and etc.Input should be their email as text\",\n",
    "                name=\"user_info_db_search\",\n",
    "            ),\n",
    "            Tool.from_function(\n",
    "                func=search_user_subscription_on_db,\n",
    "                description=\"Database tool to search user subscription type by user id, requires a number as input,\",\n",
    "                name=\"user_subscription_db_search\",\n",
    "            ),\n",
    "        ]\n",
    "        return tools\n",
    "\n",
    "    def _get_message_output(\n",
    "        self, msg_input: Union[str, BaseModel]\n",
    "    ) -> List[MessageOutput]:\n",
    "        user_info = msg_input if isinstance(msg_input, str) else str(msg_input)\n",
    "        message = f\"User Info retrieved: {user_info}\"\n",
    "        return [MessageOutput]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a061f1f-00a7-4097-9298-a73119793a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BaseEdge(abc.ABC, Generic[EdgeInput, ResultsType]):\n",
      "    def __init__(self, model, max_retries=3, out_node=None):\n",
      "        self._llm_model = model\n",
      "\n",
      "        # how many times the edge has failed, for any reason, for deciding to skip\n",
      "        # when successful this resets to 0 for posterity.\n",
      "        self._num_fails = 0\n",
      "\n",
      "        # how many retrys are acceptable\n",
      "        self._max_retries = max_retries\n",
      "\n",
      "        # the node the edge directs towards\n",
      "        self._out_node = out_node\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_message_output(\n",
      "        self, msg_input: Union[str, BaseModel]\n",
      "    ) -> Optional[List[MessageOutput]]:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def check(self, model_output: str) -> bool:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _parse(self, model_input: EdgeInput) -> ResultsType:\n",
      "        pass\n",
      "\n",
      "    def _get_edge_output(\n",
      "        self, should_continue: bool, result: Optional[ResultsType]\n",
      "    ) -> EdgeOutput:\n",
      "        message_output = self._get_message_output(result)\n",
      "        return EdgeOutput(\n",
      "            should_continue=should_continue,\n",
      "            result=result,\n",
      "            num_fails=self._num_fails,\n",
      "            next_node=self._out_node,\n",
      "            message_output=message_output,\n",
      "        )\n",
      "\n",
      "    def execute(self, user_input: EdgeInput):\n",
      "        \"\"\"Executes the entire edge\n",
      "        returns a dictionary:\n",
      "        {\n",
      "            continue: bool,       weather or not should continue to next\n",
      "            result: parse_class,  the parsed result, if applicable\n",
      "            num_fails: int        the number of failed attempts\n",
      "            continue_to: Node     the Node the edge continues to\n",
      "        }\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            # attempting to parse\n",
      "            self._num_fails = 0\n",
      "            return self._get_edge_output(\n",
      "                should_continue=True, result=self._parse(user_input)\n",
      "            )\n",
      "        except OutputParserException as parsing_exception:\n",
      "            # there was some error in parsing.\n",
      "            # note, using the retry or correction parser here might be a good idea\n",
      "            self._num_fails += 1\n",
      "            if self._num_fails >= self._max_retries:\n",
      "                return self._get_edge_output(\n",
      "                    should_continue=True,\n",
      "                    result=MessageOutput(\n",
      "                        parsing_exception.llm_output, role=Role.SYSTEM\n",
      "                    ),\n",
      "                )\n",
      "            return self._get_edge_output(\n",
      "                should_continue=False,\n",
      "                result=MessageOutput(parsing_exception.llm_output, role=Role.SYSTEM),\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BaseEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c61327-af29-4275-b445-167d6822ac8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ChainBasedEdge(BaseEdge[MessageHistory, MessageOutput], ABC):\n",
      "    def __init__(\n",
      "        self,\n",
      "        model,\n",
      "        pydantic_object: Optional[Type[BaseModel]],\n",
      "        max_retries=3,\n",
      "        out_node=None,\n",
      "    ):\n",
      "        super().__init__(model=model, max_retries=max_retries, out_node=out_node)\n",
      "        if pydantic_object is not None:\n",
      "            self._output_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
      "        else:\n",
      "            self._output_parser = None\n",
      "\n",
      "        self._init_chain()\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _predict(self, model_input: ModelInput) -> str:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _init_chain(self, *kwargs):\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_prompt_template(self) -> BasePromptTemplate:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _prompt_input_variables(self) -> list:\n",
      "        pass\n",
      "\n",
      "    def check(self, model_output: str) -> bool:\n",
      "        return isinstance(self._output_parser.parse(model_output), BaseModel)\n",
      "\n",
      "    def _parse(self, message_history: MessageHistory) -> Union[str, BaseModel]:\n",
      "        model_input = message_history.model_input()\n",
      "        str_to_parse = self._predict(model_input=model_input)\n",
      "        out = (\n",
      "            self._output_parser.parse(str_to_parse)\n",
      "            if self._output_parser is not None\n",
      "            else str_to_parse\n",
      "        )\n",
      "        return out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ChainBasedEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f86b832-c419-443b-ae0e-710a624ae2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ZeroShotChainBasedEdge(ChainBasedEdge, ABC):\n",
      "    _prompt_prefix = None\n",
      "    _prompt_suffix = None\n",
      "\n",
      "    def _prompt_input_variables(self):\n",
      "        input_variables = [\"input\", \"agent_scratchpad\", \"history\"]\n",
      "        if self._output_parser is not None:\n",
      "            input_variables += [\"format_instructions\"]\n",
      "\n",
      "        return input_variables\n",
      "\n",
      "    def _get_prompt_template(self) -> BasePromptTemplate:\n",
      "        input_variables = self._prompt_input_variables()\n",
      "        history = \"\"\"Conversation History \\n{history}\\n\"\"\"\n",
      "\n",
      "        prompt = ZeroShotAgent.create_prompt(\n",
      "            tools=self._tools,\n",
      "            prefix=self._prompt_prefix,\n",
      "            suffix=history + self._prompt_suffix,\n",
      "            input_variables=input_variables,\n",
      "        )\n",
      "        return prompt\n",
      "\n",
      "    def _init_chain(self, **kwargs):\n",
      "        self._tools = self._get_tools()\n",
      "\n",
      "        self._prompt = self._get_prompt_template()\n",
      "        self._llm_chain = LLMChain(llm=self._llm_model, prompt=self._prompt)\n",
      "\n",
      "        agent = ZeroShotAgent(llm_chain=self._llm_chain, tools=self._tools)\n",
      "        self._agent_executor = AgentExecutor.from_agent_and_tools(\n",
      "            agent=agent, tools=self._tools, verbose=True, handle_parsing_errors=True\n",
      "        )\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_tools(self):\n",
      "        pass\n",
      "\n",
      "    def _predict(self, model_input: ModelInput) -> str:\n",
      "        if self._output_parser is not None:\n",
      "            result = self._agent_executor.run(\n",
      "                input=model_input.input,\n",
      "                history=model_input.history,\n",
      "                format_instructions=self._output_parser.get_format_instructions(),\n",
      "            )\n",
      "        else:\n",
      "            result = self._agent_executor.run(\n",
      "                input=model_input.input, history=model_input.history\n",
      "            )\n",
      "\n",
      "        return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ZeroShotChainBasedEdge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1edf9ec2-7e6f-498b-ac6d-730403cca81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_info_edge = UserInfoChainBasedEdge(model=llm_model, pydantic_object=UserProfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "941c63b3-78ef-41d0-a1c8-812d88dde2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the user information and their subscription type.\n",
      "Action: user_info_db_search\n",
      "Action Input: michaeljackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[{'name': 'Michael Jackson', 'email': 'michaeljackson@gmail.com', 'user_id': '1', 'phone': '0452 333 666', 'language': 'English'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the user information for michaeljackson@gmail.com. Now I need to find their subscription type.\n",
      "Action: user_subscription_db_search\n",
      "Action Input: 1\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m[{'user_id': '1', 'subscription': 'premium'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What is the user information and their subscription type for michaeljackson@gmail.com?\n",
      "Thought: I need to find the user information and their subscription type.\n",
      "Action: user_info_db_search\n",
      "Action Input: michaeljackson@gmail.com\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[{'name': 'Michael Jackson', 'email': 'michaeljackson@gmail.com', 'user_id': '1', 'phone': '0452 333 666', 'language': 'English'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the user information for michaeljackson@gmail.com. Now I need to find their subscription type.\n",
      "Action: user_subscription_db_search\n",
      "Action Input: 1\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m[{'user_id': '1', 'subscription': 'premium'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: \n",
      "{\n",
      "  \"name\": \"Michael Jackson\",\n",
      "  \"email\": \"michaeljackson@gmail.com\",\n",
      "  \"subscription\": \"premium\",\n",
      "  \"user_id\": 1,\n",
      "  \"phone\": \"0452 333 666\",\n",
      "  \"language\": \"English\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EdgeOutput(should_continue=True, result=UserProfile(name='Michael Jackson', email='michaeljackson@gmail.com', subscription='premium', user_id=1, phone='0452 333 666', language='English'), message_output=[<class 'data.graph.MessageOutput'>], num_fails=0, next_node=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info_edge.execute(get_message_history(\"michaeljackson@gmail.com\", Role.USER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5c10555-ed8c-4825-ad09-94976d362df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AuthenticatedUserNode(MultiRetrievalNode):\n",
    "    STATIC_PROMPT = [\n",
    "        \"Hi, {user_name} I am your Shopify Agent for today, you have the {subscription} subscription \"\n",
    "        \"I can help you with any Help or you can ask me to call you at anytime!\"\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model,\n",
    "        pydantic_object: Optional[Type[BaseNode]],\n",
    "        edges: List[BaseEdge] = None,\n",
    "    ):\n",
    "        self._hc_agent = HelpCenterAgent()\n",
    "        super().__init__(llm_model, pydantic_object, edges)\n",
    "\n",
    "    def greeting_message(self) -> Optional[MessageOutput]:\n",
    "        prompt = random.choice(self.STATIC_PROMPT)\n",
    "        user_profile: UserProfile = self._node_input\n",
    "\n",
    "        prompt = prompt.format(\n",
    "            user_name=user_profile.name, subscription=user_profile.subscription\n",
    "        )\n",
    "        return MessageOutput(prompt, role=Role.ASSISTANT)\n",
    "\n",
    "    def _get_retriever_infos(self):\n",
    "        retriever_infos = [\n",
    "            {\n",
    "                \"name\": \"Premium Subscription Knowledge Base\",\n",
    "                \"description\": \"Contains information for user with a premium subscription\",\n",
    "                \"retriever\": self._hc_agent.paid_sub_retriever(),\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Free Subscription Knowledge Base\",\n",
    "                \"description\": \"Contains information for user with a free subscription\",\n",
    "                \"retriever\": self._hc_agent.free_sub_retriever(),\n",
    "            },\n",
    "        ]\n",
    "        return retriever_infos\n",
    "\n",
    "    def _get_default_chain(self):\n",
    "        template = \"\"\"You are a helpful assistant, you should tell the user that his query is outside of your domain \n",
    "    in a friendly way\"\n",
    "    Human: \"\"\"\n",
    "\n",
    "        prompt_template = PromptTemplate.from_template(template)\n",
    "        chain = LLMChain(\n",
    "            llm=self._llm_model, prompt=prompt_template, output_key=\"result\"\n",
    "        )\n",
    "        return chain\n",
    "\n",
    "    def no_edges_found(self, user_input: MessageHistory) -> Optional[MessageOutput]:\n",
    "        message = self._predict(user_input)\n",
    "        return MessageOutput(message=message, role=Role.ASSISTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acc5d6bb-1317-4c03-a013-ab4d60675d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ChainBasedNode(BaseNode[MessageHistory], abc.ABC):\n",
      "    def __init__(\n",
      "        self,\n",
      "        llm_model,\n",
      "        pydantic_object: Optional[Type[BaseModel]],\n",
      "        edges: Optional[List[BaseEdge]],\n",
      "        final_state=False,\n",
      "    ):\n",
      "        self._llm_model = llm_model\n",
      "        self._parse_class = pydantic_object\n",
      "\n",
      "        if pydantic_object is not None:\n",
      "            self._output_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
      "        else:\n",
      "            self._output_parser = None\n",
      "\n",
      "        self._init_chain()\n",
      "        super().__init__(edges, final_state)\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _init_chain(self, **kwargs):\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(ChainBasedNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b433e2-0927-41f3-8a69-430b71a1fa77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class MultiRetrievalNode(ChainBasedNode, abc.ABC):\n",
      "    @abc.abstractmethod\n",
      "    def _get_retriever_infos(self):\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def _get_default_chain(self):\n",
      "        pass\n",
      "\n",
      "    def _init_chain(self, *kwargs):\n",
      "        retriever_infos = self._get_retriever_infos()\n",
      "\n",
      "        self._llm_chain = MultiRetrievalQAChain.from_retrievers(\n",
      "            self._llm_model,\n",
      "            retriever_infos,\n",
      "            default_chain=self._get_default_chain(),\n",
      "            verbose=True,\n",
      "        )\n",
      "\n",
      "    def _predict(self, messages: MessageHistory) -> str:\n",
      "        return self._llm_chain.run(messages)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(MultiRetrievalNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aa7ca62-bb01-465d-91a6-018f044aa617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "authenticated_user_node = AuthenticatedUserNode(\n",
    "    llm_model=llm_model, pydantic_object=None, edges=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21347aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRetrievalQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rpossas/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premium Subscription Knowledge Base: {'query': 'What is a POS'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MessageOutput(message=\"A POS, or point of sale, refers to the location where a transaction takes place. It can also refer to the software or system used to process sales transactions. In the context of Shopify, Shopify POS is a point of sale app that allows merchants to sell their products in person, whether it's in retail stores, pop-up shops, or other locations. The Shopify POS app is available for iOS and Android devices and syncs with Shopify to track orders and inventory across different sales channels.\", role=<Role.ASSISTANT: 'assistant'>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authenticated_user_node.execute(\n",
    "    get_message_history(\"User with Premium Subscription: What is a POS\", Role.USER)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
